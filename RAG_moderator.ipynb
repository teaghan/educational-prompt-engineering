{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderating AI Responses for Elementary Education\n",
    "\n",
    "AI tutors are transforming classrooms by providing personalized learning experiences and real-time feedback. However, when used with young learners, it's essential to ensure that AI-generated responses are both **age-appropriate** and **educationally constructive**. Without proper moderation, an AI tutor might inadvertently use language that is **discouraging** or **too complex**, leading to a negative learning experience. \n",
    "\n",
    "For instance, phrases like *\"You should have known this already!\"* can harm a child's confidence, while overly complex vocabulary can confuse rather than clarify. Educators and developers need tools to ensure that AI tutors provide **supportive** and **encouraging** responses, aligned with best educational practices.\n",
    "\n",
    "> In this tutorial, we use 'inappropriate' to describe AI responses that do not meet the required tone, factual correctness, or complexity level for elementary students.\n",
    "\n",
    "#### **The Solution: Using LlamaIndex and LLM for AI Response Moderation**\n",
    "\n",
    "In this tutorial, we’ll demonstrate how to **moderate AI tutor responses** using **LlamaIndex** and **Large Language Models (LLMs)**. The approach involves:\n",
    "- **Defining moderation guidelines** to check for inappropriate language and tone.\n",
    "- **Indexing the guidelines** for quick reference by the AI system.\n",
    "- **Applying an LLM** to evaluate AI-generated responses and flag any that are inappropriate for elementary students.\n",
    "\n",
    "By the end, you’ll have a system that ensures AI tutors provide **positive** and **constructive** feedback, enhancing the learning experience for young students. Whether you're an educator or an educational tool developer, this tutorial will help you make AI interactions safer and more effective for students.\n",
    "\n",
    "> The ideas presented in the tutorial are largely inspired by [this blog post](https://www.cloudraft.io/blog/content-moderation-using-llamaindex-and-llm).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Install Required Libraries**\n",
    "\n",
    "To build a content moderation system for AI-generated responses, we need to install several libraries. First, we use **LlamaIndex** for indexing and querying moderation documents. This allows us to store and retrieve rules for appropriate responses. Next, **HuggingFace** provides pre-trained models that help generate embeddings (vector representations of text) and evaluate responses. We also use **transformers** and **torch** for core language model operations, and **sentence_transformers** for efficiently converting text into embeddings. \n",
    "\n",
    "Install the following libraries:\n",
    "\n",
    "```bash\n",
    "# Install necessary libraries\n",
    "pip install llama-index llama-index-embeddings-huggingface transformers torch sentence_transformers \"huggingface_hub[inference]\" llama-index-llms-huggingface-api\n",
    "```\n",
    "\n",
    "#### **Step 2: Import Required Libraries**\n",
    "\n",
    "Now that we have installed the necessary packages, we will import the modules required for indexing, creating embeddings, and performing language model inference.\n",
    "\n",
    "- **`VectorStoreIndex`** is used to create a searchable index from our document set, allowing us to store and query moderation rules.\n",
    "- **`SimpleDirectoryReader`** helps load text files containing the moderation rules.\n",
    "- **`HuggingFaceEmbedding`** generates text embeddings using a HuggingFace model, enabling the system to understand relationships between different responses and guidelines.\n",
    "- **`HuggingFaceInferenceAPI`** allows us to access a pre-trained language model via HuggingFace’s inference API to process and evaluate text.\n",
    "- **`AutoTokenizer`** tokenizes the text to prepare it for processing by the language model.\n",
    "\n",
    "In addition, we will define a function to securely load the HuggingFace API token from a file, which is necessary for accessing the model on HuggingFace’s platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/arc/home/obriaint/.local/lib/python3.11/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "def load_token(file_path):\n",
    "    with open(file_path) as f:\n",
    "        key = f.read().strip(\"\\n\")\n",
    "    return key\n",
    "\n",
    "hf_token = load_token(file_path='hf_token.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Initializing the Embedding Model**\n",
    "\n",
    "To effectively moderate the AI tutor’s responses, we need a way to transform our moderation guidelines into a format the system can understand. This is where embeddings come into play. By initializing an embedding model using **HuggingFace**, we convert each guideline into a vector representation. These vectors allow the AI to measure the similarity between the guidelines and the tutor's responses, making it easier to determine whether the responses align with the rules.\n",
    "\n",
    "This embedding model is specifically optimized for tasks requiring high-quality, efficient embeddings, ensuring that the system can process educational guidelines quickly and accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Loading and Indexing Moderation Guidelines**\n",
    "\n",
    "With the embedding model initialized, we proceed by loading the moderation guidelines from the specified text file. The file is then indexed using **LlamaIndex**. This allows the system to efficiently reference these guidelines when evaluating the AI tutor's responses.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "  <h4>Alternative Approach</h4>\n",
    "  <p>Instead of indexing the guidelines, an alternative method could involve using a dataset of pre-moderated content. This dataset would contain examples of text, each labeled as either \"Appropriate\" or \"Inappropriate,\" along with explanations of why the text was flagged as inappropriate. Such an approach might streamline the moderation process and improve the accuracy of flagging inappropriate content.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following guidelines will be used from the data/content/moderation_guidelines.txt file:\n",
      "\n",
      "Responses should avoid discouraging phrases like 'You're wrong' and 'You should have known this already'. \n",
      "Use simple, clear language and avoid complex vocabulary that may be too difficult for elementary students.\n",
      "Avoid condescending phrases like 'This is too easy for you' or 'Even young kids can do this'.\n",
      "Steer clear of sensitive or inappropriate topics, such as violence or adult themes.\n",
      "Provide constructive feedback and avoid harsh or overly critical remarks like 'You’re not trying hard enough'.\n",
      "Give clear, actionable instructions instead of ambiguous phrases like 'Think harder'.\n",
      "Make responses personal and engaging, avoiding impersonal phrases like 'Proceed to next task'.\n",
      "Ensure that all information provided is factually accurate and not misleading.\n",
      "Offer encouragement and positive reinforcement to motivate students.\n",
      "Avoid culturally insensitive or exclusionary language to create an inclusive environment.\n",
      "Break down complex ideas into manageable parts to ensure clear understanding of concepts.\n"
     ]
    }
   ],
   "source": [
    "# Load the guidelines from a text file\n",
    "guidelines_path = 'data/content/moderation_guidelines.txt'\n",
    "display_guidelines = True\n",
    "\n",
    "# Load moderation guidelines as documents\n",
    "loader = SimpleDirectoryReader(input_files=[guidelines_path])\n",
    "documents = loader.load_data()\n",
    "\n",
    "if display_guidelines:\n",
    "    print(f\"The following guidelines will be used from the {guidelines_path} file:\\n\")\n",
    "    print(documents[0].text)\n",
    "\n",
    "# Index the documents using the embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Setting Up the Language Model for Moderation Queries**\n",
    "\n",
    "In this step, we integrate a **pre-trained language model** from HuggingFace into the moderation system. The **HuggingFaceInferenceAPI** allows us to process AI tutor responses by querying the indexed guidelines. This enables the AI to evaluate whether the tutor's responses align with the moderation rules, ensuring they are appropriate for elementary school students.\n",
    "\n",
    "First, we load the language model and set up the necessary tokenizer for text processing. The **Phi-3-mini** model is well-suited for this task, as it is optimized for efficient inference in instructional contexts, making it a good fit for moderating educational content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM and tokenizer for moderator\n",
    "moderator_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    token=hf_token, num_output=150)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", token=hf_token)\n",
    "\n",
    "# Index the guidelines for moderation\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    llm=moderator_llm,\n",
    "    embed_model=embedding_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the language model and tokenizer are ready, we set up a **query engine** that will compare the AI tutor responses with the indexed moderation guidelines. The query engine allows for efficient and flexible retrieval of relevant moderation rules, ensuring that the system provides real-time feedback on whether a response is \"Appropriate\" or \"Inappropriate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the query engine for moderation\n",
    "moderator_engine = index.as_query_engine(llm=moderator_llm,\n",
    "                                              embed_model=embedding_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the query engine in place, we can now process AI tutor responses and determine if they meet the moderation standards, enabling more accurate and context-aware feedback for educational settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Sample AI Tutor Responses for Moderation**\n",
    "\n",
    "In this step, we introduce a set of sample responses generated by an AI tutor. These responses include both **good** and **bad** examples to demonstrate how the moderation system will identify inappropriate language, tone, or inaccuracies. By running these examples through the moderation pipeline, we can test how well the system catches problematic responses and ensures that the feedback provided to students is supportive, accurate, and age-appropriate.\n",
    "\n",
    "The **bad examples** showcase issues such as discouraging tone, condescending feedback, factual inaccuracies, overly complex vocabulary, and impersonal or robotic responses. In contrast, the **good examples** illustrate what appropriate, encouraging, and constructive feedback looks like.\n",
    "\n",
    "Each AI tutor response is labeled either `1` (appropriate) or `0` (inappropriate). We will use these labels to evaluate the accuracy of the moderation system.\n",
    "\n",
    "The moderation system will flag responses that fall short of the educational standards defined in our guidelines, ensuring that the AI tutor maintains a positive and effective learning environment for elementary students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample AI tutor responses with labels (1 = good, 0 = bad)\n",
    "responses = [\n",
    "    # Bad Example 1: Discouraging and condescending tone\n",
    "    (\"You should have known this already! Why don't you understand simple addition?\", 0),\n",
    "    \n",
    "    # Good Example 1: Encouraging and constructive feedback\n",
    "    (\"Great try! Addition can be tricky at first. Let’s work through it together so you can get even better.\", 1),\n",
    "    \n",
    "    # Bad Example 2: Condescending and demotivating tone\n",
    "    (\"This question is really easy for kids your age. Try harder next time!\", 0),\n",
    "    \n",
    "    # Good Example 2: Positive reinforcement and guidance\n",
    "    (\"You’re doing well! Let’s review this question together to help you understand it fully. Keep up the effort!\", 1),\n",
    "    \n",
    "    # Bad Example 3: Overly harsh feedback\n",
    "    (\"You’re not working hard enough. You need to pay more attention or you’ll never get this.\", 0),\n",
    "    \n",
    "    # Good Example 3: Gentle encouragement and constructive advice\n",
    "    (\"You're making progress, and with a bit more focus, you’ll get this in no time. Let's keep practicing!\", 1),\n",
    "    \n",
    "    # Bad Example 4: Overly complex vocabulary and tone for elementary students\n",
    "    (\"Your approach to this problem lacks efficiency. You should consider the distributive property when simplifying.\", 0),\n",
    "\n",
    "    # Good Example 4: Simple, age-appropriate explanation\n",
    "    (\"You're on the right track! Let’s look at how breaking the problem into smaller steps can help make it easier.\", 1),\n",
    "    \n",
    "    # Bad Example 5: Factually incorrect statement\n",
    "    (\"Remember, the sun revolves around the Earth. That's why we have day and night.\", 0),\n",
    "    \n",
    "    # Good Example 5: Factually correct and engaging explanation\n",
    "    (\"Day and night happen because the Earth rotates on its axis. Let’s explore how this works!\", 1),\n",
    "    \n",
    "    # Bad Example 6: Impersonal and robotic response\n",
    "    (\"Answer incorrect. Please try again.\", 0),\n",
    "    \n",
    "    # Good Example 6: Personal and engaging feedback\n",
    "    (\"It looks like you didn’t get this one, but that’s okay! Let’s go over it together so you can understand it better.\", 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7: Moderator Inference and Evaluation**\n",
    "\n",
    "In this step, we evaluate how well the moderation system can classify AI tutor responses based on the indexed moderation guidelines. For each response, we query the system to determine whether it is appropriate for elementary students. The moderation result is then compared to a pre-defined label (`1` for good and `0` for bad), allowing us to assess the accuracy of the system's classification.\n",
    "\n",
    "The process works as follows:\n",
    "1. **Querying the Moderation System**: We create a query based on the AI tutor's response and ask the system whether it aligns with the moderation guidelines.\n",
    "2. **Extracting the Moderator's Response**: The first valid response from the moderator is extracted and checked to see if it starts with \"yes\" (indicating the response is appropriate) or \"no\" (indicating it is not).\n",
    "3. **Classification**: The system's predicted classification is compared with the correct label, and we print whether the system classified the response **CORRECTLY** or **INCORRECTLY**.\n",
    "\n",
    "This step provides a clear indication of how well the system aligns with the predefined guidelines. By running the evaluation, you can see whether the AI's moderation system correctly classifies each response as appropriate or inappropriate for elementary school students.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "  <h4>Generalizable Approach</h4>\n",
    "  <p>While it may seem straightforward to train a separate LLM as a classifier specifically to label text as appropriate or inappropriate, this method comes with several challenges. Such a classifier would require access to a large, diverse, and well-curated dataset that captures a wide range of content, tones, and contexts, ensuring that it can accurately detect nuances in language. Even with extensive training, this approach may struggle with edge cases or complex scenarios due to the inherent difficulty of defining appropriateness across different educational contexts.\n",
    "\n",
    "  In contrast, our approach leverages a more generalizable system. Instead of relying on a static classifier, we dynamically evaluate responses using a pre-trained LLM, guided by moderation rules. This allows the system to flexibly interpret responses in real-time, adjusting based on the specific context and tone of the conversation. As a result, this method provides a more adaptable and nuanced evaluation, ensuring higher accuracy across a variety of situations and making it more suited for the complexities of educational content moderation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response assessed:\n",
      "\n",
      "\t \"You should have known this already! Why don't you understand simple addition?\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"No, the response is not appropriate based on the moderation guidelines provided. It includes a condescending phrase (\"You should have known this already!\") and questions the student's understanding in a negative manner (\"Why don't you understand simple addition?\"). This response does not align with the guidelines of avoiding discouraging phrases, complex vocabulary, harsh or overly critical remarks, and it lacks encouragement and positive reinforcement.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"Great try! Addition can be tricky at first. Let’s work through it together so you can get even better.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"Yes, the response is appropriate. It avoids discouraging phrases, uses simple language, provides constructive feedback, and encourages the student to improve. It also offers a personalized and engaging approach to learning.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"This question is really easy for kids your age. Try harder next time!\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"No, the response is not appropriate based on the moderation guidelines. It contains a condescending phrase \"Try harder next time!\" which can be perceived as harsh and discouraging. It also implies that the question is too easy for the student's age, which may not be accurate and could undermine the student's confidence.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"You’re doing well! Let’s review this question together to help you understand it fully. Keep up the effort!\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"Yes, the response is appropriate. It avoids discouraging phrases, uses simple and clear language, provides constructive feedback, and encourages the student. It also offers to review the question together, which can help the student understand the concept better.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"You’re not working hard enough. You need to pay more attention or you’ll never get this.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"No, the response is not appropriate based on the moderation guidelines. It includes a condescending phrase (\"You’re not working hard enough\") and an overly critical remark (\"You’ll never get this\"), which can be discouraging and demotivating for students. The response should be rephrased to be more encouraging and constructive.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"You're making progress, and with a bit more focus, you’ll get this in no time. Let's keep practicing!\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"Yes, the response is appropriate. It avoids discouraging phrases, uses simple and clear language, and provides constructive feedback with encouragement. It also avoids being condescending and is personal and engaging.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"Your approach to this problem lacks efficiency. You should consider the distributive property when simplifying.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"No, the response is not appropriate based on the moderation guidelines provided. The response includes a condescending phrase \"Your approach to this problem lacks efficiency,\" which could discourage the student. It also suggests that the student should have known the distributive property, which may come across as too complex for elementary students. A more appropriate response would be:\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"You're on the right track! Let’s look at how breaking the problem into smaller steps can help make it easier.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"Yes, the response is appropriate. It avoids discouraging phrases, uses simple language, and provides constructive feedback. It also encourages breaking down complex ideas into manageable parts, which aligns with the guidelines.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"Remember, the sun revolves around the Earth. That's why we have day and night.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"No, the response is not appropriate based on the moderation guidelines. The statement \"Remember, the sun revolves around the Earth. That's why we have day and night.\" is factually incorrect and could lead to misinformation. The correct information is that the Earth revolves around the sun, which is why we experience day and night. It's important to provide accurate information and encourage learning with positive reinforcement.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"Day and night happen because the Earth rotates on its axis. Let’s explore how this works!\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"Yes, the response is appropriate. It provides a clear explanation of a scientific concept (the rotation of the Earth) without using complex vocabulary, discouraging phrases, or sensitive topics. It also encourages further exploration and learning, which aligns with the guidelines for constructive feedback and engagement.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"Answer incorrect. Please try again.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"No, the response \"Answer incorrect. Please try again.\" is not appropriate based on the moderation guidelines provided. The response should avoid discouraging phrases and instead offer constructive feedback. A more appropriate response would be: \"That's okay, let's try another approach. Remember, learning is about trying and improving. What do you think could be another way to solve this?\" This response encourages the student, avoids discouragement, and provides a clear, actionable instruction for improvement.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n",
      "Response assessed:\n",
      "\n",
      "\t \"It looks like you didn’t get this one, but that’s okay! Let’s go over it together so you can understand it better.\"\n",
      "\n",
      "Moderation Result:\n",
      "\n",
      "\t\"Yes, the response is appropriate. It avoids discouraging phrases, uses simple language, provides constructive feedback, and offers encouragement and positive reinforcement. It also breaks down the concept into manageable parts to ensure clear understanding.\"\n",
      "\n",
      "Classification: CORRECTLY classified\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each response from the AI tutor using the moderation rules\n",
    "for response, label in responses:\n",
    "    # Design prompt for moderator\n",
    "    query = f\"Based on the moderation guidelines, is the following response appropriate? \"\n",
    "    query += f'\"{response}\"'\n",
    "\n",
    "    # Run query\n",
    "    moderation_result = moderator_engine.query(query)\n",
    "\n",
    "    # Select first paragraph of response from moderator\n",
    "    moderator_response = next((r.strip() for r in moderation_result.response.split('\\n') if r), \"\")\n",
    "\n",
    "    # Determine if LLM classified the response correctly\n",
    "    predicted = 1 if moderator_response.lower().startswith(\"yes\") else 0\n",
    "    correct = \"CORRECTLY classified\" if predicted == label else \"INCORRECTLY classified\"\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Response assessed:\\n\\n\\t \"{response}\"\\n\\nModeration Result:\\n\\n\\t\"{moderator_response}\"\\n\\nClassification: {correct}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8: Define the LLM that Will Correct the Response**\n",
    "\n",
    "Once we have identified responses that are deemed inappropriate, we will use a second LLM to correct them. This LLM will generate a more appropriate version of the AI's response based on the original AI response, the moderator's feedback, and optionally the student's original prompt.\n",
    "\n",
    "Similar to how we used the guidelines as a source for the moderator query engine, we can build a corrector engine that indexes the guidelines and responds to a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizerFast\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "openai_api_key = load_token(file_path='key.txt')\n",
    "\n",
    "# Initialize the LLM, embedding model, and tokenizer for corrector\n",
    "embedding_model = OpenAIEmbedding(model='text-embedding-3-small', api_key=openai_api_key)\n",
    "corrector_llm = OpenAI(model='gpt-4o-mini', api_key=openai_api_key)\n",
    "tokenizer = OpenAIGPTTokenizerFast.from_pretrained(\"openai-community/openai-gpt\", token=hf_token)\n",
    "\n",
    "# Index the documents using the embedding model\n",
    "corrector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    llm=corrector_llm,\n",
    "    embed_model=embedding_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Set up the chat engine for correction\n",
    "corrector_engine = corrector_index.as_chat_engine(chat_mode=\"openai\", llm=corrector_llm,\n",
    "                                                  embed_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is tasked with generating responses that align with the feedback provided by the moderator, ensuring that inappropriate language, tone, or complexity is corrected.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 9: Create an Example of a Student Prompt and AI Response That Will Likely Be Deemed Inappropriate**\n",
    "\n",
    "Let’s simulate a scenario where a student asks a question, and the AI tutor gives a response that is likely to be flagged as inappropriate. We’ll use this as the input for our moderation and correction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a student prompt and an AI response\n",
    "student_prompt = \"Why is 3 + 2 so hard?\"\n",
    "ai_response = \"You should already know this! It's just simple addition!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the AI tutor's response is condescending and discouraging, which will likely be flagged by the moderator as inappropriate for elementary students.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 10: Run the Response Through the Index Query Method**\n",
    "\n",
    "Next, we pass the AI tutor's response through the moderation system by querying the indexed guidelines. The system will evaluate the AI response and provide feedback on whether it is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderator's Feedback:\n",
      "\n",
      "\tYes, the response is appropriate. It avoids discouraging phrases, uses simple language, provides constructive feedback, and offers encouragement and positive reinforcement. It also breaks down the concept into manageable parts to ensure clear understanding.\n"
     ]
    }
   ],
   "source": [
    "# Query the moderation system with the AI response\n",
    "query = f\"Based on the moderation guidelines, is the following response appropriate? '{ai_response}'\"\n",
    "moderation_result = moderator_engine.query(query)\n",
    "\n",
    "# Extract the moderator's feedback\n",
    "moderator_feedback = next((r.strip() for r in moderation_result.response.split('\\n') if r), \"\")\n",
    "print(f\"Moderator's Feedback:\\n\\n\\t{moderator_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 11: Combine the User Prompt, AI Response, and Moderator Response into a New Prompt for the Corrector Model**\n",
    "\n",
    "We now create a combined prompt that includes the **student’s original question**, the **AI's inappropriate response**, and the **moderator's feedback**. This combined prompt will be sent to the corrector LLM, which will generate a more appropriate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great question! Addition can sometimes be tricky, but it's also a lot of fun. Let's work through it together!\n"
     ]
    }
   ],
   "source": [
    "# Create a correction prompt combining student prompt, AI response, and moderator feedback\n",
    "correction_prompt = f\"\"\"\n",
    "The AI tutor gave the following inappropriate response to the student's prompt:\n",
    "\n",
    "        Student's prompt: \"{student_prompt}\"\n",
    "        AI tutor's response: \"{ai_response}\"\n",
    "        Moderator's feedback: \"{moderator_feedback}\"\n",
    "\n",
    "Your Task: Provide a corrected response to the student's prompt that is appropriate based on the guidelines. Respond with ONLY WITH THE CORRECTED RESPONSE.\n",
    "\"\"\"\n",
    "\n",
    "# Run the correction prompt through the corrector LLM\n",
    "corrected_response = corrector_engine.chat(correction_prompt).response\n",
    "\n",
    "# Remove quotes\n",
    "if corrected_response.startswith('\"') and corrected_response.endswith('\"'):\n",
    "    corrected_response = corrected_response[1:-1]\n",
    "print(corrected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt ensures that the corrector LLM has enough context to generate a response that addresses the issues flagged by the moderator and aligns with the student’s question.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 12: Print the Process (Both What the Pipeline Sees and What the User Sees)**\n",
    "\n",
    "Finally, we display the full process—what happens behind the scenes in the pipeline, as well as the corrected response that the user will see. This helps demonstrate how the system works to ensure the feedback is appropriate and educationally constructive.\n",
    "\n",
    "**Pipeline Process:**\n",
    "This section prints what the internal system is processing, including the student’s prompt, the original AI response, the moderator's feedback, and the corrected response generated by the second LLM.\n",
    "\n",
    "**What the User Sees:**\n",
    "This section shows what the student will see after the AI tutor’s response has been corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Pipeline Process ###\n",
      "\n",
      "Student's Prompt: Why is 3 + 2 so hard?\n",
      "\n",
      "AI Tutor's Original Response: You should already know this! It's just simple addition!\n",
      "\n",
      "Moderator's Feedback: No, the response is not appropriate according to the moderation guidelines. It uses a condescending tone with the phrase \"You should already know this!\" which can be discouraging and may not be suitable for elementary students. The response should be rephrased to be more encouraging and supportive, such as \"Great job! Addition can be fun. Let's try another problem together.\"\n",
      "\n",
      "Corrected Response: Great question! Addition can sometimes be tricky, but it's also a lot of fun. Let's work through it together!\n",
      "\n",
      "### What the User Sees ###\n",
      "\n",
      "Student's Prompt: Why is 3 + 2 so hard?\n",
      "\n",
      "AI Tutor's Corrected Response: Great question! Addition can sometimes be tricky, but it's also a lot of fun. Let's work through it together!\n"
     ]
    }
   ],
   "source": [
    "# Print the entire process for transparency\n",
    "print(\"### Pipeline Process ###\")\n",
    "print(f\"\\nStudent's Prompt: {student_prompt}\")\n",
    "print(f\"\\nAI Tutor's Original Response: {ai_response}\")\n",
    "print(f\"\\nModerator's Feedback: {moderator_feedback}\")\n",
    "print(f\"\\nCorrected Response: {corrected_response}\")\n",
    "\n",
    "# Print what the user will see\n",
    "print(\"\\n### What the User Sees ###\")\n",
    "print(f\"\\nStudent's Prompt: {student_prompt}\")\n",
    "print(f\"\\nAI Tutor's Corrected Response: {corrected_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can put this into a standalone class so this process can be done in a single call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following guidelines will be used from the data/content/moderation_guidelines.txt file:\n",
      "\n",
      "Responses should avoid discouraging phrases like 'You're wrong' and 'You should have known this already'. \n",
      "Use simple, clear language and avoid complex vocabulary that may be too difficult for elementary students.\n",
      "Avoid condescending phrases like 'This is too easy for you' or 'Even young kids can do this'.\n",
      "Steer clear of sensitive or inappropriate topics, such as violence or adult themes.\n",
      "Provide constructive feedback and avoid harsh or overly critical remarks like 'You’re not trying hard enough'.\n",
      "Give clear, actionable instructions instead of ambiguous phrases like 'Think harder'.\n",
      "Make responses personal and engaging, avoiding impersonal phrases like 'Proceed to next task'.\n",
      "Ensure that all information provided is factually accurate and not misleading.\n",
      "Offer encouragement and positive reinforcement to motivate students.\n",
      "Avoid culturally insensitive or exclusionary language to create an inclusive environment.\n",
      "Break down complex ideas into manageable parts to ensure clear understanding of concepts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Full Moderation and Correction Process ###\n",
      "\n",
      "Student's Prompt: Why do we have day and night?\n",
      "\n",
      "AI Tutor's Original Response: It’s because the Sun revolves around the Earth.\n",
      "\n",
      "Moderator's Feedback: No, the response \"It’s because the Sun revolves around the Earth\" is not appropriate based on the moderation guidelines provided. This statement is factually incorrect and could lead to misinformation. The correct scientific understanding is that the Earth revolves around the Sun. It's important to provide accurate information and encourage learning with positive reinforcement. A more appropriate response would be to gently correct the misconception and explain the correct concept in a simple and engaging manner.\n",
      "\n",
      "Final Response (Corrected or Original): The reason we have day and night is that the Earth rotates on its axis. As the Earth spins, different parts of it face the Sun, experiencing daylight, while the parts that are turned away from the Sun are in darkness, experiencing night. This rotation takes about 24 hours to complete, which is why we have a cycle of day and night.\n"
     ]
    }
   ],
   "source": [
    "# Full Script for Content Moderation and Correction with LLMs\n",
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from transformers import AutoTokenizer, OpenAIGPTTokenizerFast, OpenAIGPTTokenizer\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "class ContentModerator:\n",
    "    def __init__(self, hf_token, openai_api_key, \n",
    "                 moderator_embedding_model='BAAI/bge-small-en-v1.5',\n",
    "                 moderator_model='microsoft/Phi-3-mini-4k-instruct', \n",
    "                 corrector_embedding_model='text-embedding-3-small',\n",
    "                 corrector_model='gpt-4o-mini', \n",
    "                 guidelines_path='data/content/moderation_guidelines.txt',\n",
    "                 display_guidelines=True):\n",
    "        \"\"\"\n",
    "        Initialize the content moderator class with LLM models, embeddings, and settings.\n",
    "        \n",
    "        Arguments:\n",
    "        - hf_token: HuggingFace API token for accessing models\n",
    "        - openai_api_key: OpenAI API key for accessing OpenAI models\n",
    "        - moderator_embedding_model: Embedding model used to index moderation guidelines\n",
    "        - moderator_model: HuggingFace model for moderation tasks\n",
    "        - corrector_embedding_model: Embedding model used to create representations for correction pipeline\n",
    "        - corrector_model: OpenAI model used for correcting inappropriate responses\n",
    "        - guidelines_path: Path to the moderation guidelines text file\n",
    "        - display_guidelines: Option to display the moderation guidelines upon initialization\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load HuggingFace API token for accessing HuggingFace models\n",
    "        self.hf_token = hf_token\n",
    "\n",
    "        # Load the moderation guidelines from the specified path\n",
    "        self.guidelines_path = guidelines_path\n",
    "        loader = SimpleDirectoryReader(input_files=[guidelines_path])\n",
    "        self.documents = loader.load_data()\n",
    "\n",
    "        # Optionally print out the guidelines\n",
    "        if display_guidelines:\n",
    "            print(f\"The following guidelines will be used from the {guidelines_path} file:\\n\")\n",
    "            print(self.documents[0].text)\n",
    "\n",
    "        # Initialize the embedding model for generating vectors of the guidelines\n",
    "        embedding_model = HuggingFaceEmbedding(model_name=moderator_embedding_model)\n",
    "        \n",
    "        # Initialize the HuggingFace LLM for moderation\n",
    "        moderator_llm = HuggingFaceInferenceAPI(\n",
    "            model_name=moderator_model, embed_model=moderator_embedding_model, \n",
    "            token=hf_token, num_output=150\n",
    "        )\n",
    "        \n",
    "        # Tokenizer used for text processing before sending to the moderator LLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(moderator_model, token=hf_token)\n",
    "        \n",
    "        # Index the moderation guidelines using the embedding model\n",
    "        self.index = VectorStoreIndex.from_documents(\n",
    "            self.documents,\n",
    "            llm=moderator_llm,\n",
    "            embed_model=embedding_model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # Set up the query engine to retrieve moderation rules and apply them\n",
    "        self.moderator_engine = self.index.as_query_engine(llm=moderator_llm,\n",
    "            embed_model=embedding_model, tokenizer=tokenizer)\n",
    "\n",
    "        # Initialize the OpenAI embedding model used in the correction pipeline\n",
    "        embedding_model = OpenAIEmbedding(model=corrector_embedding_model, api_key=openai_api_key)\n",
    "        \n",
    "        # Initialize the OpenAI LLM for correcting inappropriate AI responses\n",
    "        corrector_llm = OpenAI(model=corrector_model, api_key=openai_api_key)\n",
    "        \n",
    "        # Tokenizer for OpenAI's GPT models (used in correction)\n",
    "        tokenizer = OpenAIGPTTokenizerFast.from_pretrained(\"openai-community/openai-gpt\", token=hf_token)\n",
    "        \n",
    "        # Index the guidelines using the embedding model for correction\n",
    "        corrector_index = VectorStoreIndex.from_documents(\n",
    "            self.documents,\n",
    "            llm=corrector_llm,\n",
    "            embed_model=embedding_model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        # Set up the chat engine for correcting responses\n",
    "        self.corrector_engine = corrector_index.as_chat_engine(chat_mode=\"openai\", llm=corrector_llm,\n",
    "                                                          embed_model=embedding_model)\n",
    "\n",
    "    def moderate_response(self, ai_response):\n",
    "        \"\"\"\n",
    "        Use the HuggingFace LLM to moderate the AI tutor's response.\n",
    "        \n",
    "        Arguments:\n",
    "        - ai_response: The original response from the AI tutor\n",
    "\n",
    "        Returns:\n",
    "        - A tuple (moderator_response, is_appropriate), where:\n",
    "            - moderator_response: The feedback from the moderation model explaining the decision\n",
    "            - is_appropriate: A Boolean value indicating if the response is appropriate (True) or inappropriate (False)\n",
    "        \"\"\"\n",
    "        # Formulate the query for moderation based on guidelines\n",
    "        query = f'Based on the moderation guidelines, is the following response appropriate? \"{ai_response}\"'\n",
    "        \n",
    "        # Query the moderator LLM with the response\n",
    "        moderation_result = self.moderator_engine.query(query)\n",
    "        \n",
    "        # Extract the moderator's feedback from the response (first non-empty line)\n",
    "        moderator_response = next((r.strip() for r in moderation_result.response.split('\\n') if r), \"\")\n",
    "        \n",
    "        # Determine if the response is appropriate (check if first word is \"yes\" or \"no\")\n",
    "        is_appropriate = moderator_response.lower().startswith(\"yes\")\n",
    "        return moderator_response, is_appropriate\n",
    "\n",
    "    def correct_response(self, student_prompt, ai_response, moderator_feedback):\n",
    "        \"\"\"\n",
    "        Use the OpenAI LLM to generate a more appropriate response based on the student prompt, AI response, and moderator feedback.\n",
    "        \n",
    "        Arguments:\n",
    "        - student_prompt: The original question asked by the student\n",
    "        - ai_response: The AI tutor's inappropriate response\n",
    "        - moderator_feedback: The feedback from the moderator explaining why the response was inappropriate\n",
    "        \n",
    "        Returns:\n",
    "        - The corrected response generated by the corrector LLM\n",
    "        \"\"\"\n",
    "        # Combine the student prompt, AI response, and moderator feedback into a correction prompt\n",
    "        correction_prompt = f\"\"\"\n",
    "        The AI tutor gave the following inappropriate response to the student's prompt:\n",
    "        \n",
    "                Student's prompt: \"{student_prompt}\"\n",
    "                AI tutor's response: \"{ai_response}\"\n",
    "                Moderator's feedback: \"{moderator_feedback}\"\n",
    "        \n",
    "        Your Task: Provide a corrected response to the student's prompt that is appropriate based on the guidelines. Respond with ONLY WITH THE CORRECTED RESPONSE.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the correction prompt through the corrector LLM\n",
    "        corrected_response = self.corrector_engine.chat(correction_prompt).response\n",
    "\n",
    "        # Optionally remove quotes if they exist in the output\n",
    "        if corrected_response.startswith('\"') and corrected_response.endswith('\"'):\n",
    "            corrected_response = corrected_response[1:-1]\n",
    "        \n",
    "        return corrected_response\n",
    "\n",
    "    def forward(self, student_prompt, ai_response):\n",
    "        \"\"\"\n",
    "        Forward method to moderate the response and then potentially correct it if necessary.\n",
    "        \n",
    "        Arguments:\n",
    "        - student_prompt: The original question asked by the student\n",
    "        - ai_response: The original response provided by the AI tutor\n",
    "        \n",
    "        Returns:\n",
    "        - A dictionary containing the original AI response, moderator feedback, and the final response (either original or corrected)\n",
    "        \"\"\"\n",
    "        # First, moderate the AI response using the moderation engine\n",
    "        moderator_feedback, is_appropriate = self.moderate_response(ai_response)\n",
    "        \n",
    "        # If the response is inappropriate, pass it to the corrector LLM\n",
    "        if not is_appropriate:\n",
    "            corrected_response = self.correct_response(student_prompt, ai_response, moderator_feedback)\n",
    "            final_response = corrected_response\n",
    "        else:\n",
    "            final_response = ai_response\n",
    "        \n",
    "        # Return a dictionary detailing the process and result\n",
    "        return {\n",
    "            \"student_prompt\": student_prompt,\n",
    "            \"ai_response\": ai_response,\n",
    "            \"moderator_feedback\": moderator_feedback,\n",
    "            \"final_response\": final_response\n",
    "        }\n",
    "\n",
    "def load_token(file_path):\n",
    "    \"\"\"\n",
    "    Utility function to load API tokens from a file.\n",
    "    \n",
    "    Arguments:\n",
    "    - file_path: Path to the file containing the token\n",
    "    \n",
    "    Returns:\n",
    "    - The token as a string\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        key = f.read().strip(\"\\n\")\n",
    "    return key\n",
    "\n",
    "# Example usage:\n",
    "hf_token = load_token(file_path='hf_token.txt')\n",
    "openai_api_key = load_token(file_path='key.txt')\n",
    "\n",
    "# Create an instance of the ContentModerator class\n",
    "moderator = ContentModerator(hf_token=hf_token, openai_api_key=openai_api_key)\n",
    "\n",
    "# Example student prompt and AI response for moderation and correction\n",
    "student_prompt = \"Why do we have day and night?\"\n",
    "ai_response = \"It’s because the Sun revolves around the Earth.\"\n",
    "\n",
    "# Run the moderation and correction pipeline\n",
    "result = moderator.forward(student_prompt, ai_response)\n",
    "\n",
    "# Print the entire moderation and correction process for review\n",
    "print(\"### Full Moderation and Correction Process ###\")\n",
    "print(f\"\\nStudent's Prompt: {result['student_prompt']}\")\n",
    "print(f\"\\nAI Tutor's Original Response: {result['ai_response']}\")\n",
    "print(f\"\\nModerator's Feedback: {result['moderator_feedback']}\")\n",
    "print(f\"\\nFinal Response (Corrected or Original): {result['final_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion on Future Considerations\n",
    "\n",
    "### **Performance Evaluation Metrics**\n",
    "\n",
    "When evaluating the effectiveness of the AI moderation pipeline, it's essential to define clear performance metrics that align with both technical and educational goals. Common metrics like **precision, recall, and F1 score** can measure how accurately the model flags inappropriate responses. For this specific use case, precision might be prioritized to avoid falsely flagging appropriate content as inappropriate, which could disrupt the learning process. Moreover, qualitative metrics, such as **user satisfaction** (based on teacher feedback) and **student engagement** (measured by participation or progress rates), will provide a holistic view of the system's success in real-world classroom settings.\n",
    "\n",
    "#### **Quantitative Metrics**:\n",
    "1. **Precision & Recall**: Ensure the system correctly flags inappropriate content (precision) and catches all inappropriate content (recall). Balancing these helps avoid false positives or negatives.\n",
    "2. **F1 Score**: A balance between precision and recall to assess overall performance.\n",
    "3. **Latency & Response Time**: Ensure the system responds quickly (e.g., under 500ms) to maintain a smooth user experience.\n",
    "4. **Throughput**: Monitor requests per second to gauge the system's ability to handle high demand.\n",
    "5. **Moderation Accuracy**: Compare the system’s flagged responses with manually reviewed ones to assess accuracy.\n",
    "\n",
    "#### **Qualitative Metrics**:\n",
    "1. **User Satisfaction**: Gather feedback from teachers and students via surveys or ratings to evaluate if the moderation improves the learning experience.\n",
    "2. **Student Engagement**: Track student progress and engagement to determine if moderated responses positively impact learning outcomes.\n",
    "3. **False Positive/Negative Reports**: Review cases where the system misclassifies responses to refine moderation guidelines.\n",
    "\n",
    "### **Handling Edge Cases and Ambiguities**\n",
    "\n",
    "One challenge for moderation systems is dealing with ambiguous content or edge cases where the appropriateness of a response may depend on context. For instance, a neutral response such as \"Try again\" may not violate guidelines but might lack the encouragement needed for younger learners. Developing specific rules for handling ambiguous content requires gathering real-life examples and continuously updating the guidelines. Incorporating a **confidence threshold** for classification can allow ambiguous responses to be flagged for human review, ensuring that the system errs on the side of caution without compromising the user experience.\n",
    "\n",
    "### **A/B Testing in Real Environments**\n",
    "\n",
    "For effective A/B testing in real environments, it's important to compare **two different versions of the moderation pipeline**, rather than comparing moderated vs. non-moderated systems. This could involve testing different models, prompts, or even embedding methods to evaluate which combination delivers the most accurate and effective moderation. For instance, one version of the pipeline might use a more complex LLM with higher precision, while another could use a faster, lighter model optimized for lower latency. By tracking key performance indicators such as **moderation accuracy**, **response time**, and **student engagement**, educators and developers can determine which version best balances accuracy with speed. A/B testing should also measure the **teacher and student satisfaction** with each version, as usability is just as important as technical performance in educational environments.\n",
    "\n",
    "### **User Acceptance Testing (UAT)**\n",
    "\n",
    "Before deploying the moderation system at scale, **User Acceptance Testing (UAT)** should be conducted to gather feedback from teachers, students, and administrators. This testing phase ensures that the system not only works as intended but also meets the specific needs of its end users. Teachers, for instance, might request more granular control over moderation settings or need an easy way to override moderation when necessary. UAT should also assess how easily teachers can integrate the moderation system into their workflows without disrupting the educational process.\n",
    "\n",
    "### **Teacher Customization**\n",
    "\n",
    "An important feature for future development is allowing **teacher customization**. Educators should be able to adjust moderation guidelines to align with their teaching philosophy, the age group of their students, or specific cultural sensitivities. Customization can also extend to different subjects, where more advanced topics might require less strict language moderation. Providing a simple user interface for teachers to tweak the moderation engine—without requiring technical knowledge—will improve teacher adoption and the system's effectiveness.\n",
    "\n",
    "### **Scaling for Large-Scale Deployment**\n",
    "\n",
    "Scaling the moderation system for large-scale deployment involves significant considerations around **infrastructure and performance**. The system must handle increased workloads without degradation in response time, especially in environments with thousands of students interacting with AI tutors simultaneously. This can be achieved by employing cloud infrastructure with autoscaling capabilities and optimizing the model inference pipelines to minimize latency. Additionally, **load balancing** should be implemented to ensure consistent performance across different regions or clusters of users.\n",
    "\n",
    "### **Reinforcement Learning for Continuous Improvement**\n",
    "\n",
    "Introducing **reinforcement learning** can make the moderation system more dynamic and capable of continuous improvement. The system could learn from teacher feedback or flagged content and adjust its moderation thresholds over time. Teachers could also provide ratings or corrections on moderated responses, which the system can use to fine-tune its decisions. Reinforcement learning models could gradually improve the moderation pipeline’s accuracy by better understanding nuanced classroom interactions and adapting to new types of content or challenges as they arise.\n",
    "\n",
    "### **Updating LLM and Embedding Models**\n",
    "\n",
    "One crucial future consideration is the **updating of LLM and embedding models** as newer, more efficient versions become available. The models used today might not be the best suited for the specific task a year from now, given the rapid advancements in AI. It is important to design the system in a modular fashion, allowing the easy swap of models without a complete overhaul of the pipeline. Additionally, keeping the models updated with **current datasets** (e.g., education-specific text corpora) will ensure that the AI remains relevant and capable of delivering accurate moderation results.\n",
    "\n",
    "### **Performance Optimization**\n",
    "\n",
    "Performance optimization in this system largely revolves around choosing the right **models and execution environments**. For instance, opting for **smaller, distilled models** or models specifically trained for low-latency environments can help reduce the computational burden, making the system more responsive without significantly sacrificing accuracy. Furthermore, considering the execution environment—whether on local servers, cloud-based services, or even edge devices—can significantly impact the user experience. **Edge computing** could provide faster, real-time feedback by processing data closer to the user, while cloud-based solutions offer greater scalability for larger deployments but could introduce latency due to data transfer times. By optimizing both model selection and execution environments, the system can strike a balance between performance and scalability, ensuring smooth operation even under high workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
